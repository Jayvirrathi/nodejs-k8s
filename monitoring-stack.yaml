# =========================
# PERSISTENT VOLUME CLAIMS
# (adjust storageClassName if needed)
# =========================
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: loki-pvc
  namespace: default
spec:
  accessModes: ["ReadWriteOnce"]
  resources:
    requests:
      storage: 10Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: prometheus-pvc
  namespace: default
spec:
  accessModes: ["ReadWriteOnce"]
  resources:
    requests:
      storage: 20Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: tempo-pvc
  namespace: default
spec:
  accessModes: ["ReadWriteOnce"]
  resources:
    requests:
      storage: 10Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mimir-pvc
  namespace: default
spec:
  accessModes: ["ReadWriteOnce"]
  resources:
    requests:
      storage: 30Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: grafana-pvc
  namespace: default
spec:
  accessModes: ["ReadWriteOnce"]
  resources:
    requests:
      storage: 5Gi

# =========================
# LOKI (CONFIG + DEPLOY + SVC)
# =========================
apiVersion: v1
kind: ConfigMap
metadata:
  name: loki-config
  namespace: default
data:
  loki-config.yml: |
    auth_enabled: false

    server:
      http_listen_port: 3100

    common:
      path_prefix: /loki
      storage:
        filesystem:
          chunks_directory: /loki/chunks
          rules_directory: /loki/rules
      replication_factor: 1
      ring:
        instance_addr: 127.0.0.1
        kvstore:
          store: inmemory

    schema_config:
      configs:
      - from: 2023-01-01
        store: boltdb-shipper
        object_store: filesystem
        schema: v13
        index:
          prefix: index_
          period: 24h

    storage_config:
      boltdb_shipper:
        active_index_directory: /loki/index
        cache_location: /loki/boltdb-cache
        shared_store: filesystem

    ruler:
      storage:
        type: local
        local:
          directory: /loki/rules
      ring:
        kvstore:
          store: inmemory
      rule_path: /loki/rules-temp
      # If you add Alertmanager later, update this URL:
      alertmanager_url: http://alertmanager.default.svc.cluster.local:9093
      enable_alertmanager_v2: true

    compactor:
      working_directory: /loki/boltdb-shipper-compactor
      shared_store: filesystem
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: loki
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: loki
  template:
    metadata:
      labels:
        app: loki
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 10001
        fsGroup: 10001
      containers:
      - name: loki
        image: grafana/loki:2.9.8
        args:
          - -config.file=/etc/loki/loki-config.yml
        ports:
        - containerPort: 3100
          name: http
        readinessProbe:
          httpGet:
            path: /ready
            port: http
          initialDelaySeconds: 10
          periodSeconds: 10
        livenessProbe:
          httpGet:
            path: /ready
            port: http
          initialDelaySeconds: 10
          periodSeconds: 10
        resources:
          requests:
            cpu: "100m"
            memory: "256Mi"
          limits:
            cpu: "500m"
            memory: "1Gi"
        volumeMounts:
        - name: loki-config
          mountPath: /etc/loki
        - name: loki-data
          mountPath: /loki
      volumes:
      - name: loki-config
        configMap:
          name: loki-config
      - name: loki-data
        persistentVolumeClaim:
          claimName: loki-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: loki
  namespace: default
spec:
  type: NodePort
  selector:
    app: loki
  ports:
  - name: http
    port: 3100
    targetPort: 3100
    nodePort: 30100

# =========================
# PROMETHEUS (RBAC + DEPLOY + SVC)
# =========================
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus
rules:
- apiGroups: [""]
  resources: ["nodes", "nodes/proxy", "services", "endpoints", "pods"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["discovery.k8s.io"]
  resources: ["endpointslices"]
  verbs: ["get", "list", "watch"]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
- kind: ServiceAccount
  name: prometheus
  namespace: default
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: default
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s

    # Send metrics to Mimir (Prometheus-compatible remote_write).
    remote_write:
      - url: http://mimir.default.svc.cluster.local:9009/api/v1/push

    scrape_configs:
      - job_name: 'kubernetes-service-endpoints'
        kubernetes_sd_configs:
        - role: endpoints
        relabel_configs:
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
          action: replace
          target_label: __scheme__
          regex: (https?)
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
          action: replace
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
          target_label: __address__
        - action: labelmap
          regex: __meta_kubernetes_service_label_(.+)
        - source_labels: [__meta_kubernetes_namespace]
          target_label: kubernetes_namespace
        - source_labels: [__meta_kubernetes_service_name]
          target_label: kubernetes_name

      - job_name: 'my-demo1-app'
        kubernetes_sd_configs:
        - role: endpoints
        relabel_configs:
        - source_labels: [__meta_kubernetes_service_name]
          action: keep
          regex: my-demo1-app
        - source_labels: [__meta_kubernetes_endpoint_port_name]
          action: keep
          regex: metrics
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      serviceAccountName: prometheus
      securityContext:
        runAsNonRoot: true
        runAsUser: 65534
        fsGroup: 65534
      containers:
      - name: prometheus
        image: prom/prometheus:v2.53.0
        args:
          - --config.file=/etc/prometheus/prometheus.yml
          - --storage.tsdb.path=/prometheus
          - --storage.tsdb.retention.time=7d
          - --web.enable-lifecycle
        ports:
        - containerPort: 9090
          name: web
        livenessProbe:
          httpGet:
            path: /-/healthy
            port: web
          initialDelaySeconds: 10
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /-/ready
            port: web
          initialDelaySeconds: 10
          periodSeconds: 10
        resources:
          requests:
            cpu: "100m"
            memory: "256Mi"
          limits:
            cpu: "500m"
            memory: "1Gi"
        volumeMounts:
        - name: config
          mountPath: /etc/prometheus
        - name: data
          mountPath: /prometheus
      volumes:
      - name: config
        configMap:
          name: prometheus-config
      - name: data
        persistentVolumeClaim:
          claimName: prometheus-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: prometheus
  namespace: default
spec:
  type: NodePort
  selector:
    app: prometheus
  ports:
  - name: web
    port: 9090
    targetPort: 9090
    nodePort: 30090

# =========================
# TEMPO (CONFIG + DEPLOY + SVC)
# =========================
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: tempo-config
  namespace: default
data:
  tempo.yaml: |
    server:
      http_listen_port: 3200
      grpc_listen_port: 9095

    distributor:
      receivers:
        otlp:
          protocols:
            grpc:
            http:

    ingester:
      max_block_duration: 5m

    compactor:
      shared_store: filesystem
      working_directory: /var/tempo/blocks

    storage:
      trace:
        backend: local
        local:
          path: /var/tempo/traces
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tempo
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: tempo
  template:
    metadata:
      labels:
        app: tempo
    spec:
      securityContext:
        runAsNonRoot: true
      containers:
      - name: tempo
        image: grafana/tempo:2.4.1
        args:
          - -config.file=/etc/tempo/tempo.yaml
        ports:
        - containerPort: 3200
          name: http
        - containerPort: 9095
          name: grpc
        readinessProbe:
          httpGet:
            path: /ready
            port: http
          initialDelaySeconds: 10
          periodSeconds: 10
        livenessProbe:
          httpGet:
            path: /ready
            port: http
          initialDelaySeconds: 10
          periodSeconds: 10
        resources:
          requests:
            cpu: "100m"
            memory: "256Mi"
          limits:
            cpu: "500m"
            memory: "512Mi"
        volumeMounts:
        - name: tempo-config
          mountPath: /etc/tempo
        - name: tempo-data
          mountPath: /var/tempo
      volumes:
      - name: tempo-config
        configMap:
          name: tempo-config
      - name: tempo-data
        persistentVolumeClaim:
          claimName: tempo-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: tempo
  namespace: default
spec:
  type: NodePort
  selector:
    app: tempo
  ports:
  - name: http
    port: 3200
    targetPort: 3200
    nodePort: 30200
  - name: grpc
    port: 9095
    targetPort: 9095
    nodePort: 30201

# =========================
# MIMIR (CONFIG + DEPLOY + SVC)
# =========================
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: mimir-config
  namespace: default
data:
  mimir.yaml: |
    multitenancy_enabled: false
    server:
      http_listen_port: 9009
    blocks_storage:
      backend: filesystem
      filesystem:
        dir: /data/mimir/tsdb
    ruler_storage:
      backend: filesystem
      filesystem:
        dir: /data/mimir/rules
    compactor:
      data_dir: /data/mimir/compactor
    ingester:
      lifecycler:
        ring:
          kvstore:
            store: inmemory
          replication_factor: 1
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mimir
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mimir
  template:
    metadata:
      labels:
        app: mimir
    spec:
      securityContext:
        runAsNonRoot: true
      containers:
      - name: mimir
        image: grafana/mimir:2.12.0
        args:
          - -config.file=/etc/mimir/mimir.yaml
        ports:
        - containerPort: 9009
          name: http
        readinessProbe:
          httpGet:
            path: /ready
            port: http
          initialDelaySeconds: 10
          periodSeconds: 10
        livenessProbe:
          httpGet:
            path: /ready
            port: http
          initialDelaySeconds: 10
          periodSeconds: 10
        resources:
          requests:
            cpu: "100m"
            memory: "256Mi"
          limits:
            cpu: "500m"
            memory: "1Gi"
        volumeMounts:
        - name: mimir-config
          mountPath: /etc/mimir
        - name: mimir-data
          mountPath: /data/mimir
      volumes:
      - name: mimir-config
        configMap:
          name: mimir-config
      - name: mimir-data
        persistentVolumeClaim:
          claimName: mimir-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: mimir
  namespace: default
spec:
  type: NodePort
  selector:
    app: mimir
  ports:
  - name: http
    port: 9009
    targetPort: 9009
    nodePort: 30099

# =========================
# OPENTELEMETRY COLLECTOR (CONFIG + DEPLOY + SVC)
# Receives OTLP from apps and forwards:
# - traces -> Tempo (otlp grpc)
# - logs   -> Loki (loki exporter)
# - metrics-> Mimir (prometheusremotewrite)
# =========================
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-config
  namespace: default
data:
  otel-collector-config.yaml: |
    receivers:
      otlp:
        protocols:
          grpc: { endpoint: 0.0.0.0:4317 }
          http: { endpoint: 0.0.0.0:4318 }
    processors:
      batch: {}
      memory_limiter:
        check_interval: 5s
        limit_mib: 512
        spike_limit_mib: 256
      resource:
        attributes:
          - key: k8s.cluster.name
            action: upsert
            value: demo-cluster
    exporters:
      otlp/tempo:
        endpoint: tempo.default.svc.cluster.local:9095
        tls: { insecure: true }
      loki:
        endpoint: http://loki.default.svc.cluster.local:3100/loki/api/v1/push
        labels:
          attributes:
            # promote common log attributes to labels
            service.name: service
            k8s.namespace.name: namespace
            k8s.pod.name: pod
            k8s.container.name: container
        # optional: tenant ID if using multi-tenancy
        # tenant_id: "tenant-1"
      prometheusremotewrite/mimir:
        endpoint: http://mimir.default.svc.cluster.local:9009/api/v1/push
        tls: { insecure: true }
    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [memory_limiter, batch, resource]
          exporters: [otlp/tempo]
        metrics:
          receivers: [otlp]
          processors: [memory_limiter, batch, resource]
          exporters: [prometheusremotewrite/mimir]
        logs:
          receivers: [otlp]
          processors: [memory_limiter, batch, resource]
          exporters: [loki]
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otel-collector
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: otel-collector
  template:
    metadata:
      labels:
        app: otel-collector
    spec:
      containers:
      - name: otel-collector
        image: otel/opentelemetry-collector-contrib:0.103.0
        args:
          - --config=/etc/otelcol/otel-collector-config.yaml
        ports:
        - containerPort: 4317
          name: otlp-grpc
        - containerPort: 4318
          name: otlp-http
        readinessProbe:
          httpGet: { path: /, port: 13133 }
          initialDelaySeconds: 10
          periodSeconds: 10
        livenessProbe:
          httpGet: { path: /, port: 13133 }
          initialDelaySeconds: 10
          periodSeconds: 10
        resources:
          requests: { cpu: "100m", memory: "256Mi" }
          limits: { cpu: "500m", memory: "512Mi" }
        volumeMounts:
        - name: otel-config
          mountPath: /etc/otelcol
      volumes:
      - name: otel-config
        configMap:
          name: otel-collector-config
---
apiVersion: v1
kind: Service
metadata:
  name: otel-collector
  namespace: default
spec:
  selector:
    app: otel-collector
  ports:
  - name: otlp-grpc
    port: 4317
    targetPort: 4317
  - name: otlp-http
    port: 4318
    targetPort: 4318
  type: ClusterIP

# =========================
# GRAFANA (PROVISIONED DATASOURCES + DEPLOY + SVC)
# =========================
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-datasources
  namespace: default
  labels:
    grafana_datasource: "1"
data:
  prometheus-and-loki.yaml: |
    apiVersion: 1
    datasources:
    - name: Prometheus
      type: prometheus
      access: proxy
      url: http://prometheus.default.svc.cluster.local:9090
      isDefault: true
      jsonData:
        timeInterval: 15s
    - name: Loki
      type: loki
      access: proxy
      url: http://loki.default.svc.cluster.local:3100
      jsonData:
        maxLines: 1000
  tempo-and-mimir.yaml: |
    apiVersion: 1
    datasources:
    - name: Tempo
      type: tempo
      access: proxy
      url: http://tempo.default.svc.cluster.local:3200
    - name: Mimir
      # Grafana uses the Prometheus data source type for Mimir's query-frontend
      type: prometheus
      access: proxy
      url: http://mimir.default.svc.cluster.local:9009
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 472
        fsGroup: 472
      containers:
      - name: grafana
        image: grafana/grafana:10.4.3
        ports:
        - containerPort: 3000
          name: web
        env:
        - name: GF_SECURITY_ADMIN_USER
          value: "admin"
        - name: GF_SECURITY_ADMIN_PASSWORD
          value: "admin"
        readinessProbe:
          httpGet:
            path: /login
            port: web
          initialDelaySeconds: 10
          periodSeconds: 10
        livenessProbe:
          httpGet:
            path: /api/health
            port: web
          initialDelaySeconds: 10
          periodSeconds: 10
        volumeMounts:
        - name: grafana-datasources
          mountPath: /etc/grafana/provisioning/datasources
        - name: grafana-data
          mountPath: /var/lib/grafana
      volumes:
      - name: grafana-datasources
        configMap:
          name: grafana-datasources
      - name: grafana-data
        persistentVolumeClaim:
          claimName: grafana-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: grafana
  namespace: default
spec:
  type: NodePort
  selector:
    app: grafana
  ports:
  - name: web
    port: 3000
    targetPort: 3000
    nodePort: 30030
